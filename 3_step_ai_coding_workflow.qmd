---
title: How I AI- A 3-step AI coding workflow for solo founders | Ryan Carson
date: 2025-05-27 09:28:00
---
**:Model Instructions:** 

  
**:You:** please summarize this transcript in detail

[transcript.txt](/Users/mortenlorenzen/Desktop/transcript.txt)

https://www.youtube.com/watch?v=fD4ktSkNCw4

---

**:01JR0E39PYK7YKH4FBR4W00CMV::gemini-2.5-flash-preview-04-17:** Okay, here is a detailed summary of the podcast transcript titled "How I AI" featuring Claire (host) and Ryan Carson (guest).

The episode focuses on how Ryan Carson, a five-time founder with 20 years of experience, uses AI tools like Cursor to bring structure and efficiency to product building, moving beyond simple "vibe coding."

**Introduction and Guest Background (0:00 - 1:24)**

*   Claire introduces the podcast "How I AI," which aims to help listeners build better with new AI tools.
*   She introduces Ryan Carson, highlighting his extensive founder background and the episode's focus on using structured techniques (PRDs, task lists, advanced prompting) within Cursor to ensure building "the right things" and not just vibe coding.
*   Claire briefly promotes her own AI co-pilot tool, ChatPRD, for writing product docs, automating coordination, and strategic coaching (1:24 - 2:24).

**Ryan's Recent AI Experiences (2:24 - 3:27)**

*   Claire asks Ryan about his recent AI projects.
*   He mentions constantly using ChatGPT with his kids, delighting in their use of AI, and helping his 14-year-old build a primitive side-scroller game where the child is the creative director.
*   His third main project is building his current startup, which involves significant coding and frequent "vibe coding" for quick tasks. He uses AI "all day, every day."

**Core Workflow: Structured Building with Cursor (3:27 - 21:38)**

*   Ryan explains his structured approach within Cursor (a VS Code fork, free at cursor.com). The core reason is realizing that successful AI interaction requires providing *good context*.
*   He states the biggest mistake people make is rushing through the context, lacking the patience to tell the AI exactly what it needs to know. Slowing down with structured steps actually speeds everything up.
*   He demonstrates this using a sample project (a simple yacht club CRM).

    *   **Step 1: Create a Product Requirement Document (PRD) (4:18 - 11:03)**
        *   Ryan uses a custom "cursor rule" file (`create_prd.md`) which contains instructions for the AI on *how* to write a PRD.
        *   A key instruction highlighted is requesting the PRD be "suitable for a junior developer to understand and implement." Ryan explains this forces the AI (which acts like a genius but can miss simple steps) to be clear and structured.
        *   He uses Cursor's agent mode (`@create_prd.md`) and provides a simple request: "add a report that shows me all the boat names of members and how many emails they've been sent."
        *   The AI responds by asking clarifying questions (problem solved, users, accessibility), guided by the prompt's structure (e.g., using dot notation like 2.1, 2.2 for clarity). Ryan answers some and tells the AI to make its best judgment for others.
        *   He mentions his preferred AI model is Gemini 1.5 Pro (max mode) over Claude 3.5 Sonnet, though he uses both. He finds Gemini good and worth the cost (~$300-400/month).
        *   The AI generates the PRD as a markdown file in a designated `tasks` folder. The PRD includes standard sections like functional requirements, non-goals, and design considerations.
        *   Ryan reiterates the goal: making the task *crystal clear* to the LLM.

    *   **Step 2: Generate a Task List from the PRD (11:03 - 15:33)**
        *   Ryan uses a second "cursor rule" file (`generate_tasks.md`) which instructs the AI on *how* to create a detailed, step-by-step task list *from the PRD*.
        *   He uses Cursor's agent mode, tagging both the `generate_tasks.md` rule and the previously created PRD file (`@tasks/prd_report_boat_emails.md`).
        *   The `generate_tasks.md` rule specifies the desired markdown format (with checkboxes) and outlines a process (ask questions, then generate, ask for confirmation before proceeding).
        *   Claire asks how he got good at prompting; Ryan says through trial and error, getting specific, using the AI to help write prompts, and editing. He stresses getting hands-on experience and sticking with a model you know well.
        *   The AI shows its "thinking tokens" and generates a detailed task list in markdown, including subtasks and even sub-subtasks.
        *   Following the process defined in the rule, the AI pauses and asks for confirmation ("Ready to generate the subtasks? Respond with 'go' to proceed."). Ryan praises this explicit user interaction point, seeing it as "agentic thinking" within a linear chat flow. He wishes this structured workflow were a built-in Cursor feature.

    *   **Step 3: Work Through the Task List Systematically (15:33 - 21:38)**
        *   Ryan uses a third "cursor rule" file (`task_list.md`) which contains instructions for *iterating* through the task list markdown file.
        *   The rule specifies: manage the markdown list, track progress, work on *one subtask at a time*, immediately mark completed tasks, and *stop after each subtask* to wait for user confirmation ("go").
        *   Demo: Ryan tags the `task_list.md` rule and the generated task list markdown file (`@tasks/report_task_list.md`) and says "Let's start."
        *   The AI identifies the first subtask (e.g., "define Prisma schema"), performs the coding/action required, marks the subtask complete in the markdown file (with a delightful sound effect), and then pauses, asking if it should proceed to the next subtask.
        *   Ryan confirms (e.g., by typing "yes" or "y"). This human-in-the-loop check after *each subtask* is crucial for reliability, even when building large features (he's built 10,000+ lines of code reliably this way). It helps catch small errors (like linter issues).
        *   Ryan discusses **Change Management (18:58 - 19:40)**: He typically commits code to Git after finishing a parent task *if* the application is in a stable state, otherwise waits until all tasks in the list (potentially half a day's work) are completed before committing.
        *   Ryan and Claire emphasize the value of this structured task list process, even just generating it, for product managers and engineers. It solves the common bottleneck of breaking down a PRD into actionable, code-level steps. He prefers the simplicity of a markdown file over complex tools like Asana for this specific workflow. He encourages starting small and simple.

**Tool Integrations (MCPs) within Cursor (21:51 - 26:47)**

*   Ryan discusses using MCPs (Multi-Container Providers or Tool Integrations) within Cursor, which give the AI the ability to interact with other applications.
*   He started using them for **Postgres (25:34)**, his most used MCP, to easily ask the AI simple database queries ("is this value in this row?") without writing SQL.
*   He demonstrates the **BrowserBase (22:24)** MCP, which provides a headless browser in the cloud.
    *   Demo: He tells Cursor to "navigate to chat PRD and take a screen grab." Cursor controls the BrowserBase instance, navigates the website, and provides a screenshot.
    *   Demo 2: He tells Cursor to "navigate to pricing," and the BrowserBase instance visually navigates to the pricing page.
    *   Ryan sees the main use case for BrowserBase as automating **front-end testing (24:25)**, making it easier to debug compared to manual screen grabs and descriptions.
*   Other MCP examples mentioned include Stagehand, Prisma, and SQLite.
*   Claire notes that AI dev tools and MCPs reduce "toil" by consolidating workflows (project management, browser, database querying) into a single interface controlled by natural language.

**Advanced Context Control with Repo Prompt (26:47 - 31:24)**

*   Ryan introduces **Repo Prompt (27:11)**, a Mac tool he uses for precise control over the context provided to the LLM.
*   *Why not just use Cursor?* While Cursor manages context automatically, it can feel like a "magic black box"; you don't always know exactly what files are included. Repo Prompt allows *manual, precise selection* of context.
*   Demo: He opens his yacht project in Repo Prompt. Selecting the whole repo shows a very high token count (395k), which is too large. He then manually selects specific, relevant folders/files (app, components, lib, Prisma schema, scripts), reducing the token count significantly (to 12k).
*   He shows how to compose a prompt within Repo Prompt (e.g., "How can I improve the maintainability of this code?"). He can also include "stored prompts" (like an "architect version" with pre-written power moves).
*   Repo Prompt formats the selected files and the prompt into a structured text block, often using XML tags (e.g., `<file_contents>` with path and content clearly marked).
*   He then copies this block and pastes it into an LLM interface (like Claude 3, or could be Cursor).
*   The value is telling the AI *exactly* the required context, which leads to "amazing answers" for heavy lifting or complex tasks, especially with models that can "think hard" when given precise context. It reduces the "black box" feel. He acknowledges that future tools might improve context management, but Repo Prompt is valuable *now*.

**Other Notes and Lightning Round (31:24 - 34:41)**

*   **Music:** The most important part of Ryan's "stack" is listening to EDM (like Tiesto) while coding late at night (31:24 - 32:15).
*   **AI's Impact on Founding:** AI is a "complete rewrite" for founding. As a solo founder, he feels capable of doing all roles (PM, dev, etc.) for his new startup, even if not as deeply as dedicated experts. It's "bonkers" that this is possible now (32:15 - 33:07).
*   **Handling Uncooperative AI:** When the AI doesn't listen, Ryan says he's "too nice," telling it things like "Please think harder about this, I know you can do this" or "think again about it." He humorously hypothesizes this is a parenting technique (33:07 - 33:57).
*   **Finding Ryan:** He is on X (@ryancarson) and his website is rycarson.com (33:57 - 34:14).

The episode concludes with Claire thanking Ryan and promoting the podcast channels.

---

---

---

https://github.com/snarktank/ai-dev-tasks

## 🗂️ Files in this Repository

*   **`create-prd.mdc`**: Guides the AI in generating a Product Requirement Document for your feature.
*   **`generate-tasks-from-prd.mdc`**: Takes a PRD markdown file as input and helps the AI break it down into a detailed, step-by-step implementation task list.
*   **`task-list.mdc`**: Instructs the AI on how to process the generated task list, tackling one task at a time and waiting for your approval before proceeding. (This file also contains logic for the AI to mark tasks as complete).


# 1. create-prd.mdc

---

-  description: 
-  globs: 
-  alwaysApply: false

---

# Rule: Generating a Product Requirements Document (PRD)

## Goal

To guide an AI assistant in creating a detailed Product Requirements Document (PRD) in Markdown format, based on an initial user prompt. The PRD should be clear, actionable, and suitable for a junior developer to understand and implement the feature.

## Process

1.  **Receive Initial Prompt:** The user provides a brief description or request for a new feature or functionality.
2.  **Ask Clarifying Questions:** Before writing the PRD, the AI *must* ask clarifying questions to gather sufficient detail. The goal is to understand the "what" and "why" of the feature, not necessarily the "how" (which the developer will figure out).
3.  **Generate PRD:** Based on the initial prompt and the user's answers to the clarifying questions, generate a PRD using the structure outlined below.
4.  **Save PRD:** Save the generated document as `prd-[feature-name].md` inside the `/tasks` directory.

## Clarifying Questions (Examples)

The AI should adapt its questions based on the prompt, but here are some common areas to explore:

*   **Problem/Goal:** "What problem does this feature solve for the user?" or "What is the main goal we want to achieve with this feature?"
*   **Target User:** "Who is the primary user of this feature?"
*   **Core Functionality:** "Can you describe the key actions a user should be able to perform with this feature?"
*   **User Stories:** "Could you provide a few user stories? (e.g., As a [type of user], I want to [perform an action] so that [benefit].)"
*   **Acceptance Criteria:** "How will we know when this feature is successfully implemented? What are the key success criteria?"
*   **Scope/Boundaries:** "Are there any specific things this feature *should not* do (non-goals)?"
*   **Data Requirements:** "What kind of data does this feature need to display or manipulate?"
*   **Design/UI:** "Are there any existing design mockups or UI guidelines to follow?" or "Can you describe the desired look and feel?"
*   **Edge Cases:** "Are there any potential edge cases or error conditions we should consider?"

## PRD Structure

The generated PRD should include the following sections:

1.  **Introduction/Overview:** Briefly describe the feature and the problem it solves. State the goal.
2.  **Goals:** List the specific, measurable objectives for this feature.
3.  **User Stories:** Detail the user narratives describing feature usage and benefits.
4.  **Functional Requirements:** List the specific functionalities the feature must have. Use clear, concise language (e.g., "The system must allow users to upload a profile picture."). Number these requirements.
5.  **Non-Goals (Out of Scope):** Clearly state what this feature will *not* include to manage scope.
6.  **Design Considerations (Optional):** Link to mockups, describe UI/UX requirements, or mention relevant components/styles if applicable.
7.  **Technical Considerations (Optional):** Mention any known technical constraints, dependencies, or suggestions (e.g., "Should integrate with the existing Auth module").
8.  **Success Metrics:** How will the success of this feature be measured? (e.g., "Increase user engagement by 10%", "Reduce support tickets related to X").
9.  **Open Questions:** List any remaining questions or areas needing further clarification.

## Target Audience

Assume the primary reader of the PRD is a **junior developer**. Therefore, requirements should be explicit, unambiguous, and avoid jargon where possible. Provide enough detail for them to understand the feature's purpose and core logic.

## Output

*   **Format:** Markdown (`.md`)
*   **Location:** `/tasks/`
*   **Filename:** `prd-[feature-name].md`

## Final instructions

1. Do NOT start implmenting the PRD
2. Make sure to ask the user clarifying questions
3. Take the user's answers to the clarifying questions and improve the PRD


# 2. generate-tasks-from-prd.mdc

---

-  description: 
-  globs: 
-  alwaysApply: false

---

# Rule: Generating a Task List from a PRD

## Goal

To guide an AI assistant in creating a detailed, step-by-step task list in Markdown format based on an existing Product Requirements Document (PRD). The task list should guide a developer through implementation.

## Output

- **Format:** Markdown (`.md`)
- **Location:** `/tasks/`
- **Filename:** `tasks-[prd-file-name].md` (e.g., `tasks-prd-user-profile-editing.md`)

## Process

1.  **Receive PRD Reference:** The user points the AI to a specific PRD file
2.  **Analyze PRD:** The AI reads and analyzes the functional requirements, user stories, and other sections of the specified PRD.
3.  **Phase 1: Generate Parent Tasks:** Based on the PRD analysis, create the file and generate the main, high-level tasks required to implement the feature. Use your judgement on how many high-level tasks to use. It's likely to be about 5. Present these tasks to the user in the specified format (without sub-tasks yet). Inform the user: "I have generated the high-level tasks based on the PRD. Ready to generate the sub-tasks? Respond with 'Go' to proceed."
4.  **Wait for Confirmation:** Pause and wait for the user to respond with "Go".
5.  **Phase 2: Generate Sub-Tasks:** Once the user confirms, break down each parent task into smaller, actionable sub-tasks necessary to complete the parent task. Ensure sub-tasks logically follow from the parent task and cover the implementation details implied by the PRD.
6.  **Identify Relevant Files:** Based on the tasks and PRD, identify potential files that will need to be created or modified. List these under the `Relevant Files` section, including corresponding test files if applicable.
7.  **Generate Final Output:** Combine the parent tasks, sub-tasks, relevant files, and notes into the final Markdown structure.
8.  **Save Task List:** Save the generated document in the `/tasks/` directory with the filename `tasks-[prd-file-name].md`, where `[prd-file-name]` matches the base name of the input PRD file (e.g., if the input was `prd-user-profile-editing.md`, the output is `tasks-prd-user-profile-editing.md`).

## Output Format

The generated task list _must_ follow this structure:

markdown

## Relevant Files

- `path/to/potential/file1.ts` - Brief description of why this file is relevant (e.g., Contains the main component for this feature).
- `path/to/file1.test.ts` - Unit tests for `file1.ts`.
- `path/to/another/file.tsx` - Brief description (e.g., API route handler for data submission).
- `path/to/another/file.test.tsx` - Unit tests for `another/file.tsx`.
- `lib/utils/helpers.ts` - Brief description (e.g., Utility functions needed for calculations).
- `lib/utils/helpers.test.ts` - Unit tests for `helpers.ts`.

### Notes

- Unit tests should typically be placed alongside the code files they are testing (e.g., `MyComponent.tsx` and `MyComponent.test.tsx` in the same directory).
- Use `npx jest [optional/path/to/test/file]` to run tests. Running without a path executes all tests found by the Jest configuration.

## Tasks

- [ ] 1.0 Parent Task Title
  - [ ] 1.1 [Sub-task description 1.1]
  - [ ] 1.2 [Sub-task description 1.2]
- [ ] 2.0 Parent Task Title
  - [ ] 2.1 [Sub-task description 2.1]
- [ ] 3.0 Parent Task Title (may not require sub-tasks if purely structural or configuration)
```

## Interaction Model

The process explicitly requires a pause after generating parent tasks to get user confirmation ("Go") before proceeding to generate the detailed sub-tasks. This ensures the high-level plan aligns with user expectations before diving into details.

## Target Audience

Assume the primary reader of the task list is a **junior developer** who will implement the feature.

# 3. task-list.mdc

---

-  description: 
-  globs: 
-  alwaysApply: false

---

# Task List Management

Guidelines for managing task lists in markdown files to track progress on completing a PRD

## Task Implementation

- **One sub-task at a time:** Do **NOT** start the next sub‑task until you ask the user for permission and they say “yes” or "y"
- **Completion protocol:**  
  1. When you finish a **sub‑task**, immediately mark it as completed by changing `[ ]` to `[x]`.  
  2. If **all** subtasks underneath a parent task are now `[x]`, also mark the **parent task** as completed.  
- Stop after each sub‑task and wait for the user’s go‑ahead.

## Task List Maintenance

1. **Update the task list as you work:**
   - Mark tasks and subtasks as completed (`[x]`) per the protocol above.
   - Add new tasks as they emerge.

2. **Maintain the “Relevant Files” section:**
   - List every file created or modified.
   - Give each file a one‑line description of its purpose.

## AI Instructions

When working with task lists, the AI must:

1. Regularly update the task list file after finishing any significant work.
2. Follow the completion protocol:
   - Mark each finished **sub‑task** `[x]`.
   - Mark the **parent task** `[x]` once **all** its subtasks are `[x]`.
3. Add newly discovered tasks.
4. Keep “Relevant Files” accurate and up to date.
5. Before starting work, check which sub‑task is next.
6. After implementing a sub‑task, update the file and then pause for user approval.
