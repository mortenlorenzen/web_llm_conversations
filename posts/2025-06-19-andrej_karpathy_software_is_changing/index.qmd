---
title: "Andre Karpathy: Software is changing"
description: |
  Andre Karpathy's talk on the evolution of software in the era of AI, discussing Software 1.0, 2.0, and 3.0, the role of LLMs, and the future of programming.
categories:
  - "AI Technology"
  - "Andrej Karpathy"   
date: 2025-06-19
image: software_is_changing.png
---

**:Model Instructions:** 

  ---

[Andrej Karpathy: Software Is Changing (Again)](https://www.youtube.com/watch?v=LCEmiRjPEtQ)
  
  
**:You:** please summarize this transcript in detail.

[trscr14.txt](/Users/mortenlorenzen/Desktop/trscr14.txt)

---

Slides: [Andrej Karpathy: Software Is Changing (Again)](andrej_karpathy_ycaistartup2025_compressed.pdf)

**:01JR0E39PYK7YKH4FBR4W00CMV::gemini-2.5-flash-preview-04-17:** Here is a detailed summary of Andre Karpathy's talk "Software in the Era of AI":

The talk begins by welcoming Andre Karpathy, former director of AI at Tesla. Karpathy expresses excitement about the current era for students entering the tech industry, stating that software is fundamentally changing again, more rapidly than it has in 70 years, with two significant shifts occurring recently. This creates a vast amount of work in writing and rewriting software.

Karpathy introduces a framework for understanding this evolution:

1.  **Software 1.0:** This is the traditional code written by humans, such as C++. It represents instructions for computers in the digital space. He points to tools like "map of GitHub" visualizing this vast amount of code.
2.  **Software 2.0:** This refers to neural networks, specifically their weights, which are *not* written directly by humans. Instead, they are created by running optimizers on large datasets. Initially seen just as classifiers, Karpathy framed them as a new type of software. He notes platforms like Hugging Face as the equivalent of GitHub for Software 2.0 models. He cites the example of the AlexNet image recognizer. At Tesla, the Autopilot's neural network stack (Software 2.0) grew in capability and size over time, literally "eating through" the C++ code (Software 1.0) as functionalities were migrated to the neural net, demonstrating 2.0's capability to replace 1.0.
3.  **Software 3.0:** This is the newest paradigm, driven by Large Language Models (LLMs). Karpathy sees LLMs as a *new kind of programmable computer*. Prompts are now programs written in natural language, often English, to program the LLM. This is remarkable because English is a native human language, making programming much more accessible. He notes GitHub repositories increasingly contain English interspersed with code, signaling this shift.

Karpathy argues that those entering the industry need to be fluent in all three paradigms (1.0, 2.0, 3.0) as each has pros and cons, and decisions must be made about which functionality belongs in which paradigm, requiring fluid transitions between them.

He then delves into how to think about LLMs and their ecosystem, drawing analogies:

*   **Utilities:** Building on Andrew Ng's "AI is the new electricity" quote, LLM labs (OpenAI, Gemini, Anthropic) spend significant capital (capex) to train models (building the grid) and operational expenses (opex) to serve intelligence via APIs (distributing electricity) through metered access (paying per token). Users demand low latency, high uptime, and consistent quality, similar to electricity. Tools like OpenRouter act as a "transfer switch" between different LLM sources. He notes that when state-of-the-art LLMs go down, it feels like an "intelligence brownout," highlighting society's growing reliance.
*   **Fabs:** The large capex for training LLMs also draws parallels to building semiconductor fabs. LLM labs centralize deep tech R&D secrets. However, the analogy is muddy because software is more malleable and less physically defensible than hardware. He uses analogies like process nodes (cluster max flops) and the fabless vs. integrated model (using Nvidia GPUs vs. Google's TPUs).
*   **Operating Systems (Most fitting):** Karpathy believes LLMs function increasingly like operating systems. They are not simple commodities but complex software ecosystems. The market structure resembles OSes: a few closed-source providers (Windows, Mac OS -> OpenAI, Gemini) and an open-source alternative (Linux -> Llama ecosystem). LLMs orchestrate memory (context windows) and compute for problem-solving, similar to a CPU managing resources. The "downloading an app" analogy applies: downloading VS Code on an OS is like running an LLM app (e.g., Cursor) on different LLM APIs (GPT, Claude).

Drawing historical parallels, Karpathy suggests we are in the **computing era circa the 1960s**. LLM compute is expensive, forcing centralization in the cloud with users acting as thin clients interacting via the network (like time-sharing and batching on early mainframes). Personal computing for LLMs hasn't fully arrived, though early signs exist (e.g., Mac minis for inference). Interacting with an LLM via text terminal feels like the early OS terminal interfaces; a general LLM GUI hasn't been invented yet.

A unique aspect of LLMs compared to past transformative technologies (electricity, cryptography, flight) is the **flipped direction of technology diffusion**. Usually, governments and corporations are the first users due to cost and complexity, with diffusion later reaching consumers. With LLMs, consumers were the first users (e.g., asking how to boil an egg), and corporations/governments are lagging in adoption. This informs where the first applications are emerging.

He then explores LLM "Psychology":

*   LLMs are seen as "people spirits," stochastic simulations of humans created by autoregressive transformers trained on human data.
*   **Superpowers:** Encyclopedic knowledge and memory (recalling hashes, like the Rainman analogy).
*   **Cognitive Deficits:** Hallucination, poor self-knowledge, jagged intelligence (superhuman in some areas, makes trivial errors in others), anterograde amnesia (fixed weights, context window is working memory wiped each interaction, doesn't natively learn/consolidate over time, like Memento/50 First Dates).
*   **Security Issues:** Gullibility, prompt injection, data leakage.

The challenge is to programmatically work with these highly useful but fallible systems, leveraging superpowers while working around deficits.

This leads to opportunities: **Partial Autonomy Apps**. Instead of interacting directly with the LLM terminal, build dedicated applications on top. Examples:

*   **Cursor:** A coding IDE built on LLMs. Users don't copy/paste to/from ChatGPT but use Cursor's integrated features.
*   **Perplexity:** A search/research tool.

Key properties of these LLM apps:

*   Handle **context management** for the user.
*   **Orchestrate multiple LLM calls** (e.g., embedding models, chat models, diff models).
*   Provide **application-specific GUIs**. This is crucial because visual interfaces (like red/green diffs) are faster for human verification and auditing than reading text. They utilize the brain's "computer vision GPU."
*   Offer an **Autonomy Slider**: Humans control the level of AI intervention (e.g., code completion, changing a chunk, changing a file, full repo agent). The user tunes autonomy based on task complexity.

Karpathy stresses the importance of the **Generation/Verification Loop**: AI generates, humans verify. The goal is to make this loop as fast as possible.

*   **Speed up Verification:** Use GUIs and visual representations.
*   **Keep AI on the Leash:** Avoid overwhelming outputs (e.g., 10k line diffs). Humans are still the bottleneck for verification. Work in small, incremental chunks.

He shares personal practices for AI-assisted coding (small diffs, fast loops) and notes that best practices for prompting (e.g., being concrete to reduce failed verification) are emerging. In education, AI can be kept "on the leash" by having a teacher app create an auditable course artifact (syllabus, projects) that a student app then serves.

He relates this to his experience with Tesla Autopilot, a partial autonomy product with a GUI showing AI perception and an evolving autonomy slider. Despite a perfect demo drive in 2013, full self-driving is still being worked on 12 years later, highlighting the difficulty of robust autonomy in complex domains like driving and software. He cautions against declaring the "year of agents," suggesting it will be the "decade of agents" requiring human-in-the-loop interaction due to software's inherent trickiness.

The **Iron Man suit analogy** encapsulates the goal: build augmentations (human-driven with AI help) and agents (AI is more autonomous). At this stage with fallible LLMs, the focus should be on building "suits" (partial autonomy products with fast gen/verify loops and GUIs) rather than purely autonomous "robots" or flashy demos. The long-term vision is sliding the autonomy from left (human-driven) to right (AI-driven).

Another unique dimension is that **LLMs are programmed in English**, making software highly accessible. Karpathy calls this "vibe coding" – winging it in natural language. He shares examples: building a basic iOS app in Swift without knowing Swift and creating Menu Gen (menu.app) which generates images for menu items. He notes that the coding part of these projects was surprisingly easy due to LLMs, but making them "real" (adding authentication, payments, deployment, domain names) was hard and time-consuming, involving manual GUI clicking and following instructions written *for humans* ("click here, go there").

This highlights a final opportunity: **Build for Agents**. LLMs are a new category of consumers and manipulators of digital information, alongside humans (GUIs) and programs (APIs). Software infrastructure needs to accommodate these "people spirits" on the internet.

*   Make information agent-accessible: Use `lm.txt` (like `robots.txt`) – a simple markdown file describing a domain for LLMs to read directly, avoiding error-prone HTML parsing.
*   Rewrite documentation: Transition human-oriented docs (lists, bold, pictures) to markdown (LLM-friendly). Companies like Vercel and Stripe are early movers.
*   Change instructions: Replace human instructions like "click this" with agent-executable commands like `curl` (Vercel example). Protocols like Anthropic's Model Context Protocol are emerging.
*   Build ingestion tools: Tools like `get-ingest` (concatenating GitHub repo files into a single text) or Deep Wiki (analyzing repos to build doc pages) make data LLM-consumable by simply changing a URL.

Even if future agents can "click," meeting LLMs halfway by providing accessible data formats is currently more efficient and necessary for the vast amount of existing software.

In summary, Karpathy concludes that it's an amazing time to enter the industry, requiring fluency in Software 1.0, 2.0, and 3.0. LLMs behave like utilities, fabs, and especially early operating systems (circa 1960s), but are uniquely accessible to everyone due to English programming ("vibe coding"). They are fallible "people spirits" requiring careful collaboration. The opportunities lie in building partial autonomy products with GUIs and autonomy sliders, focusing on fast human verification loops, and adapting infrastructure and documentation to be readable and executable by agents. The next decade will see the autonomy slider move from human-driven to AI-driven.

