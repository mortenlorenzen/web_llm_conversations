---
title: "Make my Own LLM Evals"
description: |
  This is a general workflow to create custom LLM Evals based on the steps outlined in the video. It includes generating outputs, manual annotation, categorizing feedback, defining evaluation criteria, creating LLM judges, implementing automated evaluation, and refining the agent's prompt.
categories:
  - "Building Stuff"
  - "Evals"
date: 2025-06-11
---


YouTube Video from Yaron Been: [How To Make Your Own LLM Evals (Must Watch For Building AI AGENTS)](https://www.youtube.com/watch?v=3jGQVdCulQI)

**:Model Instructions:** 

  ---

  
**:You:** please summarize the steps used in this video so i can use it as a general workflow to Make my Own LLM Evals .

[trscr10.txt](/Users/mortenlorenzen/Desktop/trscr10.txt)

---

**:01JR0E39PYK7YKH4FBR4W00CMV::gemini-2.5-flash-preview-04-17:** Okay, here is a summary of the steps outlined in the video for creating custom LLM Evals, which you can use as a general workflow:

1.  **Generate or Collect Outputs:** Start by having your AI agent (or LLM) generate a batch of outputs for the specific task you want to evaluate (e.g., cold emails, customer support responses, code snippets). The video used 20 cold email templates as an example.

2.  **Manual Annotation & Feedback ("Reward Reflow"):** Manually review each generated output yourself (or have domain experts do it). Provide honest, detailed feedback on each one, noting what works, what doesn't, specific issues, and general impressions.
    *   *Tip:* Stop annotating when you start noticing the same feedback categories recurring frequently.

3.  **Categorize Feedback (Using an LLM):** Collect all the manual feedback comments. Use another LLM (like ChatGPT or Claude) to analyze this raw feedback and group similar points into distinct categories (e.g., Tone Issues, Clarity of Value Prop, Subject Line Problems, Call to Action Strength, Personalization).
    *   *Refinement:* You can ask the LLM to start very specific and then group the categories if they are too granular.

4.  **Define Evaluation Categories/Criteria:** Based on the categorized feedback, finalize the key aspects or dimensions you will use to evaluate outputs going forward. These become your custom evaluation criteria.

5.  **Create LLM Judges (Eval Prompts):** For each defined evaluation category (or a logical grouping of categories), write a detailed prompt for an LLM. This prompt instructs the LLM on how to act as a "judge" for that specific aspect. It should tell the LLM:
    *   Its persona (e.g., "expert copy editor focused on cold emails").
    *   Its task (e.g., "judge the tone of the email below").
    *   Specific criteria to look for (based on your categories, e.g., "remove em dashes," "ensure line breaks," "highlight buzzwords").
    *   The desired output format (e.g., a short verdict, problematic phrases, a rating scale, a pass/fail).

6.  **Implement Automated Evaluation:** Use the LLM judges you created to automatically evaluate the outputs generated by your agent. This involves feeding the agent's output and the judge's prompt to an LLM and recording the judge's response for each output.

7.  **Define Pass/Fail Rubric:** Establish a clear rule or set of rules based on the LLM judge's output (ratings, verdicts, specific keywords) to determine whether an output is considered a "Pass" or "Fail" for production or further use. This is the "bottom line" check.

8.  **Utilize Evaluation Results:**
    *   **Filter:** Use the Pass/Fail rubric to filter out outputs that don't meet your quality standards.
    *   **Iterate (Optional):** For failed outputs, you can potentially feed the LLM judge's feedback back to the agent and ask it to regenerate a better version.
    *   **Improve the Agent's Prompt:** Analyze the common failure modes identified by your evals (especially from the initial manual annotation and categorized feedback). Use these insights to refine the *initial prompt* you give to your agent to guide it towards generating outputs that meet your quality standards from the start (e.g., explicitly telling the agent "Do not use em dashes" if that was a frequent issue).

9.  **Build/Use an Annotation Tool (Optional but Recommended):** To make the manual annotation process (Step 2) and ongoing review easier, especially for teams or non-technical users, consider building or using a simple front-end tool where users can review outputs, provide feedback, and apply labels/ratings based on your criteria. This streamlines data collection for future analysis and eval improvement.

By following these steps, you move from subjective human feedback to structured, categorized criteria, which you then automate using LLM judges and a clear Pass/Fail metric, enabling you to systematically improve the reliability of your AI agent's outputs for your specific use case.

