---
title: "Devin Kearns: 9 Things I Wish I Knew Before Starting an AI Company"
description: |
  Devin Kearns shares nine key lessons learned from his experience building an AI company over the past 8-9 months. The video covers practical insights on agentic prompting, model selection, custom tooling and the importance of UI/UX.
categories:
  - "AI in Business"
date: 2025-05-26 
image: devin_kearns_9_things_ai_company.png
---

**:Model Instructions:** 


  
**:You:** please summarize this video in detail

[![9 Things I Wish I Knew Before Starting an AI Company](https://i.ytimg.com/vi_webp/iMzEzLZ9gXw/maxresdefault.webp)](https://www.youtube.com/watch?v=iMzEzLZ9gXw)

---

**:01JR0E39PYK7YKH4FBR4W00CMV::gemini-2.5-flash-preview-04-17:** Okay, here is a detailed summary of the YouTube video "9 Things I Wish I Knew Before Starting an AI Company" by Devin Kearns from CUSTOM AI STUDIO.

The video discusses lessons learned over 8-9 months of building an AI company, focusing on practical insights for others in the space or working with AI companies. The nine key takeaways are:

1.  **Agentic Prompting is About Decision Frameworks, Not Step-by-Step Instructions:**
    *   **Old Approach:** Writing massive, overly conditional prompts trying to fit the entire decision tree, edge cases, and explicit tool call parameters into a single prompt. This was like giving step-by-step conditional instructions.
    *   **Result:** Constant hallucinations and endless prompt tweaking.
    *   **New Approach:** Hardcode deterministic logic (using `if`, `switch`, `filter` nodes) outside the LLM where possible. Structure and package input data and necessary context (like CRM records, order history) *before* it hits the LLM. Give the LLM "thinking space" within defined boundaries or "guardrails" (macro objectives, policies, business context). Prompts define *how* the LLM should make decisions and plan steps, using available tools (which the LLM discovers and uses via appended schemas, not requiring explicit parameter listing in the main prompt).
    *   **Learned:** Overspecifying leads to underperformance. Providing "guardrails" and the "why" beats lengthy "how-to" instructions most of the time. Prompts are hypotheses needing testing at scale, not code that can be verified by reading. Emotion (like "ALL CAPS DO NOT") can heighten LLM attention. Causality with prompts is slippery and hard to debug compared to code.
    *   **Example:** A customer success bot saw a 20% improvement in output appropriateness by switching from step-by-step instructions ("if this question, answer this way") to a framework focusing on the objective (decrease refunds) and providing options/guardrails for the LLM to navigate the conversation and avoid refunds where appropriate.
    *   **Roadmap:** Live KPI monitoring and autotriggering A/B prompt tests to continuously improve performance based on metrics (e.g., reducing refund rate from 8% to 7% to 6%). Prompt optimization is challenging due to the slippery nature of causality and LLMs being less effective with negative instructions ("do not do this").

2.  **Model Selection is Crucial for Production Readiness:**
    *   **Old Approach:** Just use the "best" model (initially GPT-4) for simplicity, assuming it works for everything.
    *   **New Approach:** The model choice significantly impacts whether a system is production-ready or just a cool proof-of-concept. Different models (OpenAI, Anthropic Claude, DeepMind, Grok) have different strengths (context window size, tool calling ability, conversational style, reasoning). The prompt needs to be dialed in for the *specific* model.
    *   **Learned:** The same prompt can produce a ~20% deviation in output quality depending on the model. Bad outputs don't always mean a bad prompt or a bad model; it could be the specific combination. Effective prompting techniques differ slightly per model. Long context windows (like Gemini) can sometimes replace Vector DB/RAG for simple data retrieval tasks faster and more accurately. Claude 3.7 Sonnet was good for high-level planning/reasoning (used in a planner/executor/verifier architecture); GPT-4o/5 is good for human-sounding conversational communication.
    *   **Roadmap:** Model testing to identify the best model for a scenario *before* dialing in the prompt. Fine-tuning models for domain-specific jargon/norms (e.g., legal industry) to improve accuracy and reduce the need for excessive scaffolding/guardrails. Standardizing prompting techniques based on both the scenario and the model (requires constant updates as models evolve).

3.  **Custom Tooling is Better Than Off-the-Shelf (Usually):**
    *   **Old Approach:** Spending months "tool hopping," assuming others had already built the necessary solutions. Constantly seeking a "silver bullet" off-the-shelf tool. Imposter syndrome as non-engineers.
    *   **New Approach:** Build custom tools internally for key functions like data processing, labeling, cleaning, API function calling, data retrieval, and prompt engineering. This provides full control, creates more relevant/effective solutions for clients, speeds up iteration cycles, and builds IP. A key focus is building an "agentic database."
    *   **Learned:** Many AI/ML teams face similar roadblocks. Off-the-shelf agentic tools are like SAS – helpful but don't 100% solve specific needs and force you to bend your architecture to their limitations. Nobody has it completely figured out yet; everyone is "hacking."
    *   **Takeaway:** We are the only ones who know exactly what tools we need for our specific architecture and problems, so we must build them ourselves. It's also more enjoyable.
    *   **Roadmap:** Dynamic tools (flexible, modular API calling based on API research and schemas), prompt evaluation and optimization frameworks, high-volume testing infrastructure (ingesting and processing live/historical data), API research/reference workflow (automating API doc/schema retrieval), and events data processing workflow (mapping user/agent actions over time).

4.  **It's an Even Playing Field - Skip Traditional Automation:**
    *   **Old Approach:** Assumed most businesses were already using traditional automation (Zapier, Make, RPA) and were ready for AI automation as the next step. Thought AI applications were obvious.
    *   **New Approach:** 70%+ of businesses haven't touched traditional automation platforms. They often come asking for basic automation. Agentic AI is a completely different paradigm – an "Agentic Operating System" (Agentic OS) that dynamically orchestrates automations and workflows as its tool set. LLMs can build/orchestrate automations themselves.
    *   **Learned:** Traditional automation offers 10-30% productivity gains (or maybe 100% operational efficiency if starting from scratch), but it's 2015 tech. Agentic AI enables AI-first, AI-native systems focused on *performance outcomes* (KPIs) rather than just system health or task completion. Focusing on traditional automation takes time/resources away from building Agentic systems. Agentic AI is not just an improvement on traditional automation; it's a paradigm shift.
    *   **Takeaway:** Businesses who missed the automation wave should skip directly to Agentic AI. Focus on building Agentic OSs.
    *   **Roadmap:** Stop doing traditional automations entirely. Focus only on projects centered around building Agentic systems (e.g., building a Sales Team Agent OS, then a Customer Success Agent OS, aiming for a full business Agentic OS).

5.  **Plant Your Flag - Define Your System:**
    *   **Old Approach:** Unsure of their place in the market, novelty of ideas, or common knowledge. Lacked confidence ("imposter syndrome") and followed market demand, building what clients asked for within their general sandbox. Assumed others had things figured out.
    *   **New Approach:** Gained confidence through experience. Defined their core, long-term winning system: build an "Agentic Database" (centralized, contextual, real-time, easily retrievable single source of truth) first, then build the "Agentic Layer" on top, and integrate "Dynamic Tools." The primary focus is helping clients define and capture their internal knowledge base and business logic (often trapped in people's heads or messy spreadsheets) into this database foundation. A business is essentially its data and SOPs (processes).
    *   **Learned:** Most people in the space (98% estimated) are copycats, waiting for a playbook. The space is new, everything is a hypothesis ("smoke and mirrors"). Their initial hypothesis about a master contextual database was validated. Their playbook is based on experience and what they see as inevitable trends. Building is key now, not just gathering information.
    *   **Roadmap:** Offer "AI Development Blueprints" involving a mandatory discovery process to understand client tech stack, data flow, and knowledge. This allows them to design the database schema, data pipelines, identify agent tasks, and build the prompt framework – boiling down the business logic into the LLM instructions, based on the foundation of well-structured data.

6.  **Community Building Works Best When Aligned with Your Strengths:**
    *   **Old Approach:** Followed the typical creator playbook (YouTube -> Discord community -> Course). Assumed it would run itself. Found creating tutorials and updating courses felt mandatory, uninspired, and quickly outdated. Set wrong expectations.
    *   **New Approach:** Lean into what is enjoyable and provides value: live interactions (webinars, VIP office hours). Found that learning from community members working on their own projects in different industries significantly accelerated their own understanding. Focus on sharing valuable templates ("super agent templates") with walkthroughs instead of detailed tutorials. Planning for in-person events.
    *   **Learned:** Should have started a focused community sooner. It's fun, valuable, and accelerates learning. Must be sustainable (align with preferences).
    *   **Takeaway:** Don't blindly copy the creator playbook. Design a community around your personal strengths and preferences; the right people will find it. It's not about scale, but quality interaction.
    *   **Roadmap:** Increase frequency of live hangouts/office hours. Establish a cadence for releasing valuable "super agent templates." Organize fun events (hackathons, "build of the week" showcases). Integrate embedded agents into their own platform. Move off existing community platforms (like Circle) to build their own platform for more control and a better user experience.

7.  **UI/UX is a Massive Bottleneck for Wide Adoption:**
    *   **Old Approach:** Assumed agentic systems would blend seamlessly into existing tools (Slack, Email) without needing a separate UI beyond the workflow builder (n8n).
    *   **New Approach:** UI/UX is the *most important* factor for widespread adoption. Since agents are performance/KPI-based, they need a dedicated "workspace" or destination. This UI should be seamless, accessible, differentiate the agent system, track its activity, allow human collaboration (monitoring, adjusting, pausing), and clearly display performance KPIs. The relationship should be Human + Agent collaboration, not Master and Tool, enabling siloed accountability (the agent system is responsible for the workflow outcome, not the human for the agent's specific poor output).
    *   **Learned:** Habit change is difficult; users stick to familiar interfaces (like ChatGPT, which succeeded due to its accessible UI). Agentic systems need a similar "UI chip moment" to become frictionless and drive habit change. Trust is an issue; users abandon agents after one failure, unlike forgiving ChatGPT. Current interactions treat agents as tools triggered by humans, hindering true collaboration and widespread use. Personal agents, while easy to build, aren't widely used because the UI/UX for daily interaction and habit change is missing.
    *   **Takeaway:** Unless the system owns the process end-to-end (via its UI/UX and accountability structure), it won't drive habit change or scale. The crucial collaborative UI between human and agentic entity is the missing piece bottlenecking adoption.
    *   **Roadmap:** Actively working on developing this human-agent collaborative UI/UX to unlock wider adoption.

8.  **The ROI Creates an Unstoppable Flywheel:**
    *   **Old Thought:** LLMs are just great tools that multiply individual productivity.
    *   **New Approach:** Focus exclusively on the direct ROI impact measured by KPIs. Agentic systems are performance-based and designed to redefine business outcomes. A business is a machine (Input -> Function -> Output). Agentic systems dramatically decrease the cost of the "Function" (OPEX) while increasing the quality/scale of the "Output" (performance, KPIs).
    *   **Learned:** Reducing OPEX *and* increasing performance (e.g., lower refund rates leading to higher Customer Lifetime Value - LTV) creates a powerful, exponential "flywheel." Higher LTV means a higher allowable Customer Acquisition Cost (CAC), allowing businesses to spend more on marketing and dominate their market share. This creates a massive, self-reinforcing growth loop. This effect is particularly intense right now before Agentic OS becomes the baseline industry standard. CEOs issuing "AI-first" memos understand this potential.
    *   **Takeaway:** We are not building process automation; we are building entire operating systems that will redefine industry standard margins. These margins will expand for early adopters before competitors catch up and prices/margins eventually shrink again.
    *   **Roadmap:** Implement Agentic OS team-by-team (Sales, CS, Ops) with the long-term vision of building a full business Agentic OS that serves as the foundation for starting or operating a business, providing a massive head start compared to traditional methods.

9.  **Know Your Role as an AI Consultant (or Expert):**
    *   **Old Approach:** Assumed clients/companies consulting with them wanted their broad perspective on the future of AI and felt entitled to challenge their core approach.
    *   **New Approach:** Consult only through the lens of their defined approach (Agentic OS, holistic systems). Put yourself in the client's shoes and understand *their* objective for consulting. Are they using you as a test bed to validate their existing idea? Do they want a broad playbook ("be the expert")? Or do they know what they want to build but need help *how* to build it ("be the builder")? Tailor your role accordingly. For large companies, they often want validation or help with a specific piece, not a fundamental pivot suggestion, even if you think their core project is misguided. Protect time for internal projects; don't consult on projects you believe are fundamentally the wrong approach unless it aligns with your own path or is part of a structured engagement like their discovery process.
    *   **Learned:** Clients have different needs and expectations for a consultant. Understanding their objective defines your role (helper, expert, builder). Don't waste time pushing your full vision on those who aren't ready or asking for it; focus that energy on those who *are* interested in your core approach or on building your own products.
    *   **Takeaway:** To be an effective AI consultant, understand the client's objective and mindset to determine your appropriate role in that scenario.
    *   **Roadmap:** Develop a structured discovery process for clients genuinely interested in implementing Agentic OS (this naturally includes consulting). Accept speaking/consulting engagements specifically about their holistic approach (Agentic OS, paradigm shift) rather than general AI automation. Decline consulting on individual alpha/beta projects that don't align with their core strategy.

In conclusion, the presenter emphasizes the shift from traditional automation and simple AI tools to building comprehensive, performance-driven Agentic Operating Systems layered on robust, centralized data. This requires a change in mindset regarding prompting, model selection, tool building, and business strategy, leading to a powerful, exponential growth flywheel for early adopters, and necessitates a focused approach to consulting and community building.

